{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"YOLO V1.ipynb","provenance":[{"file_id":"1PCqlip7wt3fZzm4vUtLUXGAqp_xD82hg","timestamp":1628429440884}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vcA95R45enBV"},"source":["Ejemplo tomado de https://www.maskaravivek.com/post/yolov1/\n","## Implementando YOLOv1 usando keras con tensorflow\n","\n","En este notebook implementamos YOLOv1 como descripto originalmente en\n","[You Only Look Once](https://arxiv.org/abs/1506.02640). El objetivo es replicar el modelo descripto en el paper y en entender los detalles de usar keras para resolver un problema más complejo."]},{"cell_type":"code","metadata":{"id":"GkcrTpbpAvFj"},"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt \n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z0_h8kTGfbP4"},"source":["## Pre-procesamiento de datos\n","\n","Vamos a usar [Pascal VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/) (Pascal Visual Object Classes) dado que el tamaño es más manejable y se puede correr en el colab.\n","\n","Las clases\n","\n","Primero, descargamos el dataset:"]},{"cell_type":"code","metadata":{"id":"G2Lw77yYM1lF"},"source":["!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n","!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n","\n","!tar xvf VOCtrainval_06-Nov-2007.tar\n","!tar xvf VOCtest_06-Nov-2007.tar\n","\n","!rm VOCtrainval_06-Nov-2007.tar\n","!rm VOCtest_06-Nov-2007.tar"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4crIAzvk3QOf"},"source":["El dataset `VOCtrainval_06-Nov-2007.tar` está organizado de la siguiente manera:\n","\n","* VOC2007/JPEGImages/\\<xxxxxx>.jpg: imágenes \n","* VOC2007/ImageSets/Main/\\<class>_\\<imgset>.txt: especifica en cada imagen si está o no la clase correspondiente (para cada imageset = [train, val, trainval]\n","* VOC2007/ImageSets/Layout: \\<imgset>.txt: archivos que determinan cada imagen a qué gruopo pertenece\n","* VOC2007/Annotations/\\<xxxxxx>.xml: archivo xml que determina las bounding boxes de los objetos y sus clases en cada imagen, estructura: \n","\n","`<annotation> ... \n","<size> \n","<width>[width]</width>\n","<height>[height]</height>\n","<depth>[depth]</depth>\n","</size> ... `\n","\n","`<object><name>[clase]</name> ... <bndbox><xmin>[xmin]</xmin><ymin>[ymin]</ymin>\n","<xmax>[xmin]</xmax><ymax>[ymin]</ymax></object>`\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DKQz1Q9Nf0tZ"},"source":["Las clases son:  \n","\n","* Personas: person\n","* Animales: bird, cat, cow, dog, horse, sheep\n","* Vehiculos: aeroplane, bicycle, boat, bus, car, motorbike, train\n","* Objetos de interior: bottle, chair, dining table, potted plant, sofa, tv/monitor\n","\n","\n","20 clases en total. \n","\n","\n","Ahora procesamos las annotations y obtenemos los labels ya que es más fácil consumir un archivo de texto en vez de XML."]},{"cell_type":"code","metadata":{"id":"SQMm2Nm4PwqK"},"source":["import argparse\n","import xml.etree.ElementTree as ET\n","import os\n","\n","parser = argparse.ArgumentParser(description='Build Annotations.')\n","parser.add_argument('dir', default='..', help='Annotations.')\n","\n","sets = [('2007', 'train'), ('2007', 'val'), ('2007', 'test')]\n","\n","classes_num = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5,\n","               'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11,\n","               'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16,\n","               'sofa': 17, 'train': 18, 'tvmonitor': 19}\n","\n","\n","def convert_annotation(year, image_id, f):\n","  '''\n","     input: \n","        year: año de dataset VOC\n","        image_id: id de la imagen\n","        f: archivo donde escribir la salidad\n","  '''\n","    in_file = os.path.join('VOCdevkit/VOC%s/Annotations/%s.xml' % (year, image_id))\n","    tree = ET.parse(in_file)\n","    root = tree.getroot()\n","\n","    for obj in root.iter('object'):\n","        # por cada objeto en el xml de anotacion de la imagen\n","        difficult = obj.find('difficult').text\n","        cls = obj.find('name').text # encontrar la clase\n","        classes = list(classes_num.keys())\n","        if cls not in classes or int(difficult) == 1: # descartar objetos invalidos\n","            continue\n","        cls_id = classes.index(cls) # obtener indice de la clase del objeto\n","        xmlbox = obj.find('bndbox')\n","        # obtener las dimensiones de la bounding box del objeto\n","        b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), \n","             int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n","        # escribir dimensiones e indice de la clase del objeto en la linea \n","        f.write(' ' + ','.join([str(a) for a in b]) + ',' + str(cls_id))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sEStgd47P3tQ","executionInfo":{"status":"ok","timestamp":1605898706558,"user_tz":-60,"elapsed":1770,"user":{"displayName":"Javier Kreiner","photoUrl":"","userId":"07381406432921956924"}},"outputId":"0dd9c731-e1b4-4dd0-e20d-cbc23100fc55"},"source":["for year, image_set in sets:\n","  # image_set: set de imagenes que interesa train, val, test\n","  print(year, image_set)\n","  with open(os.path.join('VOCdevkit/VOC%s/ImageSets/Main/%s.txt' % (year, image_set)), 'r') as f:\n","    # obtener la lista de imagenes en el set\n","      image_ids = f.read().strip().split()\n","  with open(os.path.join(\"VOCdevkit\", '%s_%s.txt' % (year, image_set)), 'w') as f:\n","      # por aca set hacer output de las imagenes \n","      for image_id in image_ids:\n","          f.write('%s/VOC%s/JPEGImages/%s.jpg' % (\"VOCdevkit\", year, image_id)) # escribir\n","          convert_annotation(year, image_id, f) # usar la funcion anterior \n","          f.write('\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2007 train\n","2007 val\n","2007 test\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Igue5gbjyym5"},"source":["Ahora hacemos una función que prepara la entrada y la salida. La entrada son imágenes de (448, 448, 3) y la salida son tensores de dimensión (7, 7, 30) = (S, S, \\[B\\*(1 + 4) +C\\]), donde S\\*S es el tamaño de la grilla con S= 7. B = 2 es la cantidad de bounding boxes por cada celda de la grilla, C = 20 es el número de categorías."]},{"cell_type":"code","metadata":{"id":"pB1jqPlohjT2"},"source":["import cv2 as cv\n","import numpy as np\n","\n","def read(image_path, label):\n","    image = cv.imread(image_path)\n","    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n","    image_h, image_w = image.shape[0:2]\n","    image = cv.resize(image, (448, 448)) # re-escalamos la imagen para que mida 448x448\n","    image = image / 255. # se normaliza el valor de los pixels entre 0 y 1\n","\n","    label_matrix = np.zeros([7, 7, 30])\n","    for l in label:\n","        l = l.split(',')\n","        l = np.array(l, dtype=np.int)\n","        xmin = l[0]\n","        ymin = l[1]\n","        xmax = l[2]\n","        ymax = l[3]\n","        cls = l[4]\n","        x = (xmin + xmax) / 2 / image_w # obtenemos el centro en x del objeto como proporcion de la imagen total\n","        y = (ymin + ymax) / 2 / image_h # obtenemos el centro en y del objeto como proporcion de la imagen total\n","        w = (xmax - xmin) / image_w # obtenemos la dimension x del objeto como proporcion de la imagen total\n","        h = (ymax - ymin) / image_h # obtenemos la dimension y del objeto como proporcion de la imagen total\n","        loc = [7 * x, 7 * y]\n","        loc_i = int(loc[1]) # en qué celda de la grilla 7x7 se encuentra el objecto (dimension y)\n","        loc_j = int(loc[0]) # en qué celda de la grilla 7x7 se encuentra el objecto (dimension x)\n","        y = loc[1] - loc_i # posicion del centro con respecto a la celda (dimension y) \n","        x = loc[0] - loc_j # posicion del centro con respecto a la celda (dimension x)\n","\n","        if label_matrix[loc_i, loc_j, 24] == 0:\n","            label_matrix[loc_i, loc_j, cls] = 1 # clase a la que pertence el objeto (one hot encoding)\n","            label_matrix[loc_i, loc_j, 20:24] = [x, y, w, h] # posicion del objeto en la celda\n","            label_matrix[loc_i, loc_j, 24] = 1  # objeto presente\n","\n","    return image, label_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"grP6sQWA3Shp"},"source":["## Entrenando el modelo\n","\n","Definimos un custom generator que retorna un batch de entradas y salidas.\n"]},{"cell_type":"code","metadata":{"id":"4OxTuZT23-c0"},"source":["from tensorflow import keras\n","\n","# NOTA: podría extenderse el generador para realizar data augmentation, en el paper\n","# original se introduce re escalado aleatorio y traslaciones de hasta 20% de la \n","# tamaño de la imagen original.\n","# también se distorsiona la imagen en su exposición y saturación de colores hasta\n","# un factor de 1.5 en espacio de colores HSV\n","class My_Custom_Generator(keras.utils.Sequence):\n","  \n","  def __init__(self, images, labels, batch_size):\n","    self.images = images\n","    self.labels = labels\n","    self.batch_size = batch_size\n","    \n","    \n","  def __len__(self) :\n","    return (np.ceil(len(self.images) / float(self.batch_size))).astype(np.int)\n","  \n","  \n","  def __getitem__(self, idx) :\n","    batch_x = self.images[idx * self.batch_size : (idx+1) * self.batch_size]\n","    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n","\n","    train_image = []\n","    train_label = []\n","\n","    for i in range(0, len(batch_x)):\n","      img_path = batch_x[i]\n","      label = batch_y[i]\n","      image, label_matrix = read(img_path, label)\n","      train_image.append(image)\n","      train_label.append(label_matrix)\n","    return np.array(train_image), np.array(train_label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qm15e-T_3tOa"},"source":["\n","Preparamos arrays con entradas y salidas para los sets de entrenamiento y validación."]},{"cell_type":"code","metadata":{"id":"Fe7ElYo960wW"},"source":["train_datasets = []\n","val_datasets = []\n","\n","with open(os.path.join(\"VOCdevkit\", '2007_train.txt'), 'r') as f:\n","    train_datasets = train_datasets + f.readlines()\n","with open(os.path.join(\"VOCdevkit\", '2007_val.txt'), 'r') as f:\n","    val_datasets = val_datasets + f.readlines()\n","\n","X_train = []\n","Y_train = []\n","\n","X_val = []\n","Y_val = []\n","\n","for item in train_datasets:\n","  item = item.replace(\"\\n\", \"\").split(\" \")\n","  X_train.append(item[0])\n","  arr = []\n","  for i in range(1, len(item)):\n","    arr.append(item[i])\n","  Y_train.append(arr)\n","\n","for item in val_datasets:\n","  item = item.replace(\"\\n\", \"\").split(\" \")\n","  X_val.append(item[0])\n","  arr = []\n","  for i in range(1, len(item)):\n","    arr.append(item[i])\n","  Y_val.append(arr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jF4U-gbT37YM"},"source":["Creamos instancias del generador para entrenamiento y validación."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QNt_YsQLXthS","executionInfo":{"status":"ok","timestamp":1605898738235,"user_tz":-60,"elapsed":1601,"user":{"displayName":"Javier Kreiner","photoUrl":"","userId":"07381406432921956924"}},"outputId":"fd0268d9-235e-4c6b-e1ed-2fa869ba5dc3"},"source":["batch_size = 4\n","my_training_batch_generator = My_Custom_Generator(X_train, Y_train, batch_size)\n","\n","my_validation_batch_generator = My_Custom_Generator(X_val, Y_val, batch_size)\n","\n","x_train, y_train = my_training_batch_generator.__getitem__(0)\n","x_val, y_val = my_training_batch_generator.__getitem__(0)\n","print(x_train.shape)\n","print(y_train.shape)\n","\n","print(x_val.shape)\n","print(y_val.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(4, 448, 448, 3)\n","(4, 7, 7, 30)\n","(4, 448, 448, 3)\n","(4, 7, 7, 30)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Sy_3I6Hn3-ER"},"source":["### Definir una capa de salida customizada\n","\n","Necesitamos hacer reshape de la salida del modelo así que definimos una capa de Keras customizada.\n"]},{"cell_type":"code","metadata":{"id":"2U5k_0_1QzoI"},"source":["from tensorflow import keras\n","import keras.backend as K\n","\n","class Yolo_Reshape(tf.keras.layers.Layer):\n","  def __init__(self, target_shape):\n","    super(Yolo_Reshape, self).__init__()\n","    self.target_shape = tuple(target_shape)\n","\n","  def get_config(self):\n","    config = super().get_config().copy()\n","    config.update({\n","        'target_shape': self.target_shape\n","    })\n","    return config\n","\n","  def call(self, input):\n","    # grilla de 7x7\n","    S = [self.target_shape[0], self.target_shape[1]]\n","    # cantidad de clases\n","    C = 20\n","    # cantidad de bounding boxes por celda\n","    B = 2\n","\n","    idx1 = S[0] * S[1] * C\n","    idx2 = idx1 + S[0] * S[1] * B\n","    \n","    # probabilidades de cada clase\n","    class_probs = K.reshape(input[:, :idx1], (K.shape(input)[0],) + tuple([S[0], S[1], C]))\n","    class_probs = K.softmax(class_probs)\n","\n","    # confianza de cada clase\n","    confs = K.reshape(input[:, idx1:idx2], (K.shape(input)[0],) + tuple([S[0], S[1], B]))\n","    confs = K.sigmoid(confs)\n","\n","    # bounding boxes\n","    boxes = K.reshape(input[:, idx2:], (K.shape(input)[0],) + tuple([S[0], S[1], B * 4]))\n","    boxes = K.sigmoid(boxes)\n","\n","    outputs = K.concatenate([class_probs, confs, boxes])\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HVOffcHE4Nif"},"source":["### Definimos el modelo YOLO\n","\n","Lo definimos según el paper original."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0eGd9X_7ekO","executionInfo":{"status":"ok","timestamp":1605898762647,"user_tz":-60,"elapsed":7523,"user":{"displayName":"Javier Kreiner","photoUrl":"","userId":"07381406432921956924"}},"outputId":"8246262a-b539-4edb-fc49-803f0fe3b6e6"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, InputLayer, Dropout, Flatten, Reshape, LeakyReLU\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n","from tensorflow.keras.regularizers import l2\n","\n","lrelu = LeakyReLU(alpha=0.1)\n","\n","nb_boxes=1\n","grid_w=7\n","grid_h=7\n","cell_w=64\n","cell_h=64\n","img_w=grid_w*cell_w\n","img_h=grid_h*cell_h\n","\n","model = Sequential()\n","model.add(Conv2D(filters=64, kernel_size= (7, 7), strides=(1, 1), input_shape =(img_h, img_w, 3), padding = 'same', activation=lrelu))\n","model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n","\n","model.add(Conv2D(filters=192, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n","\n","model.add(Conv2D(filters=128, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=256, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n","\n","model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n","\n","model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu))\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), strides=(2, 2), padding = 'same'))\n","\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=lrelu))\n","model.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=lrelu))\n","\n","model.add(Flatten())\n","model.add(Dense(512))\n","model.add(Dense(1024))\n","model.add(Dropout(0.5))\n","model.add(Dense(1470, activation='sigmoid'))\n","model.add(Yolo_Reshape(target_shape=(7,7,30))) # hacemos reshape de la salida\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 448, 448, 64)      9472      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 224, 224, 64)      0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 224, 224, 192)     110784    \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 112, 112, 192)     0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 112, 112, 128)     24704     \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 112, 112, 256)     295168    \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 112, 112, 256)     65792     \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 112, 112, 512)     1180160   \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 56, 56, 512)       0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 56, 56, 256)       131328    \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 56, 56, 512)       1180160   \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 56, 56, 256)       131328    \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 56, 56, 512)       1180160   \n","_________________________________________________________________\n","conv2d_10 (Conv2D)           (None, 56, 56, 256)       131328    \n","_________________________________________________________________\n","conv2d_11 (Conv2D)           (None, 56, 56, 512)       1180160   \n","_________________________________________________________________\n","conv2d_12 (Conv2D)           (None, 56, 56, 256)       131328    \n","_________________________________________________________________\n","conv2d_13 (Conv2D)           (None, 56, 56, 512)       1180160   \n","_________________________________________________________________\n","conv2d_14 (Conv2D)           (None, 56, 56, 512)       262656    \n","_________________________________________________________________\n","conv2d_15 (Conv2D)           (None, 56, 56, 1024)      4719616   \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 28, 28, 1024)      0         \n","_________________________________________________________________\n","conv2d_16 (Conv2D)           (None, 28, 28, 512)       524800    \n","_________________________________________________________________\n","conv2d_17 (Conv2D)           (None, 28, 28, 1024)      4719616   \n","_________________________________________________________________\n","conv2d_18 (Conv2D)           (None, 28, 28, 512)       524800    \n","_________________________________________________________________\n","conv2d_19 (Conv2D)           (None, 28, 28, 1024)      4719616   \n","_________________________________________________________________\n","conv2d_20 (Conv2D)           (None, 28, 28, 1024)      9438208   \n","_________________________________________________________________\n","conv2d_21 (Conv2D)           (None, 14, 14, 1024)      9438208   \n","_________________________________________________________________\n","conv2d_22 (Conv2D)           (None, 12, 12, 1024)      9438208   \n","_________________________________________________________________\n","conv2d_23 (Conv2D)           (None, 10, 10, 1024)      9438208   \n","_________________________________________________________________\n","flatten (Flatten)            (None, 102400)            0         \n","_________________________________________________________________\n","dense (Dense)                (None, 512)               52429312  \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1024)              525312    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 1024)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1470)              1506750   \n","_________________________________________________________________\n","yolo__reshape (Yolo_Reshape) (None, 7, 7, 30)          0         \n","=================================================================\n","Total params: 114,617,342\n","Trainable params: 114,617,342\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i9SGPNmS4qGu"},"source":["### Definimos una evolución del learning rate\n","\n","El paper utiliza learning rates diferentes por cada epoch. Así que definimos una función de callback para implementarlo.\n"]},{"cell_type":"code","metadata":{"id":"XLvhKEb5GKhI"},"source":["from tensorflow import keras\n","\n","class CustomLearningRateScheduler(keras.callbacks.Callback):\n","    def __init__(self, schedule):\n","        super(CustomLearningRateScheduler, self).__init__()\n","        self.schedule = schedule\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        if not hasattr(self.model.optimizer, \"lr\"):\n","            raise ValueError('El optimizador debe tener un atributo \"lr\".')\n","        # Obtener el learning rate actual del optimizer\n","        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n","        # llamar a la función que determina el learning rate que debe valer ahorra\n","        scheduled_lr = self.schedule(epoch, lr)\n","        # setear el valor que debe valer en el optimizer antes de que empiece esta epoch\n","        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n","        print(\"\\nEpoch %05d: Learning rate = %6.4f.\" % (epoch, scheduled_lr))\n","\n","\n","LR_SCHEDULE = [\n","    # tuplas de (epoch de comienzo, learning rate) \n","    (0, 0.01),\n","    (75, 0.001),\n","    (105, 0.0001),\n","]\n","\n","def lr_schedule(epoch, lr):\n","    \"\"\" Función helper para recuperar el learning rate según el schedule\"\"\"\n","    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n","        return lr\n","    for i in range(len(LR_SCHEDULE)):\n","        if epoch == LR_SCHEDULE[i][0]:\n","            return LR_SCHEDULE[i][1]\n","    return lr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q9_hD2d445zw"},"source":["### Definimos la función de pérdida\n","\n","Ahora definimos una función de pérdida customizada para usar en el modelo. Para entender más acerca de la función de pérdida leer el post [Understanding YOLO](https://hackernoon.com/understanding-yolo-f5a74bbc7967). \n","\n","La implementación de la función de pérdida fue tomada de este [repo de Github](https://github.com/JY-112553/yolov1-keras-voc)."]},{"cell_type":"code","metadata":{"id":"MN-oFCQolCUi"},"source":["import keras.backend as K\n","\n","\n","def xywh2minmax(xy, wh):\n","    xy_min = xy - wh / 2\n","    xy_max = xy + wh / 2\n","\n","    return xy_min, xy_max\n","\n","\n","def iou(pred_mins, pred_maxes, true_mins, true_maxes):\n","    intersect_mins = K.maximum(pred_mins, true_mins)\n","    intersect_maxes = K.minimum(pred_maxes, true_maxes)\n","    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n","    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n","\n","    pred_wh = pred_maxes - pred_mins\n","    true_wh = true_maxes - true_mins\n","    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n","    true_areas = true_wh[..., 0] * true_wh[..., 1]\n","\n","    union_areas = pred_areas + true_areas - intersect_areas\n","    iou_scores = intersect_areas / union_areas\n","\n","    return iou_scores\n","\n","\n","def yolo_head(feats):\n","    # implementación dinámica de dimensiones de convolución para un modelo convolucional\n","    conv_dims = K.shape(feats)[1:3]  # asumimos los canales en la última dimensión\n","    # En YOLO el índice de la altura es la iteración es la iteración central\n","    conv_height_index = K.arange(0, stop=conv_dims[0])\n","    conv_width_index = K.arange(0, stop=conv_dims[1])\n","    conv_height_index = K.tile(conv_height_index, [conv_dims[1]])\n","\n","    # conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\n","    conv_width_index = K.tile(\n","        K.expand_dims(conv_width_index, 0), [conv_dims[0], 1])\n","    conv_width_index = K.flatten(K.transpose(conv_width_index))\n","    conv_index = K.transpose(K.stack([conv_height_index, conv_width_index]))\n","    conv_index = K.reshape(conv_index, [1, conv_dims[0], conv_dims[1], 1, 2])\n","    conv_index = K.cast(conv_index, K.dtype(feats))\n","\n","    conv_dims = K.cast(K.reshape(conv_dims, [1, 1, 1, 1, 2]), K.dtype(feats))\n","\n","    box_xy = (feats[..., :2] + conv_index) / conv_dims * 448\n","    box_wh = feats[..., 2:4] * 448\n","\n","    return box_xy, box_wh\n","\n","\n","def yolo_loss(y_true, y_pred):\n","    label_class = y_true[..., :20]  # ? * 7 * 7 * 20\n","    label_box = y_true[..., 20:24]  # ? * 7 * 7 * 4\n","    response_mask = y_true[..., 24]  # ? * 7 * 7\n","    response_mask = K.expand_dims(response_mask)  # ? * 7 * 7 * 1\n","\n","    predict_class = y_pred[..., :20]  # ? * 7 * 7 * 20\n","    predict_trust = y_pred[..., 20:22]  # ? * 7 * 7 * 2\n","    predict_box = y_pred[..., 22:]  # ? * 7 * 7 * 8\n","\n","    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n","    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n","\n","    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n","    label_xy = K.expand_dims(label_xy, 3)  # ? * 7 * 7 * 1 * 1 * 2\n","    label_wh = K.expand_dims(label_wh, 3)  # ? * 7 * 7 * 1 * 1 * 2\n","    label_xy_min, label_xy_max = xywh2minmax(label_xy, label_wh)  # ? * 7 * 7 * 1 * 1 * 2, ? * 7 * 7 * 1 * 1 * 2\n","\n","    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n","    predict_xy = K.expand_dims(predict_xy, 4)  # ? * 7 * 7 * 2 * 1 * 2\n","    predict_wh = K.expand_dims(predict_wh, 4)  # ? * 7 * 7 * 2 * 1 * 2\n","    predict_xy_min, predict_xy_max = xywh2minmax(predict_xy, predict_wh)  # ? * 7 * 7 * 2 * 1 * 2, ? * 7 * 7 * 2 * 1 * 2\n","\n","    iou_scores = iou(predict_xy_min, predict_xy_max, label_xy_min, label_xy_max)  # ? * 7 * 7 * 2 * 1\n","    best_ious = K.max(iou_scores, axis=4)  # ? * 7 * 7 * 2\n","    best_box = K.max(best_ious, axis=3, keepdims=True)  # ? * 7 * 7 * 1\n","\n","    box_mask = K.cast(best_ious >= best_box, K.dtype(best_ious))  # ? * 7 * 7 * 2\n","\n","    no_object_loss = 0.5 * (1 - box_mask * response_mask) * K.square(0 - predict_trust)\n","    object_loss = box_mask * response_mask * K.square(1 - predict_trust)\n","    confidence_loss = no_object_loss + object_loss\n","    confidence_loss = K.sum(confidence_loss)\n","\n","    class_loss = response_mask * K.square(label_class - predict_class)\n","    class_loss = K.sum(class_loss)\n","\n","    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n","    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n","\n","    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n","    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n","\n","    box_mask = K.expand_dims(box_mask)\n","    response_mask = K.expand_dims(response_mask)\n","\n","    box_loss = 5 * box_mask * response_mask * K.square((label_xy - predict_xy) / 448)\n","    box_loss += 5 * box_mask * response_mask * K.square((K.sqrt(label_wh) - K.sqrt(predict_wh)) / 448)\n","    box_loss = K.sum(box_loss)\n","\n","    loss = confidence_loss + class_loss + box_loss\n","\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hgSKBlj35e5_"},"source":["### Agregar el callback para guardar los pesos"]},{"cell_type":"code","metadata":{"id":"w4Fs-f9eZXBO"},"source":["# funciona para guardar los pesos del mejor modelo\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","mcp_save = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zlwmFdWy5r1h"},"source":["### Compilar el modelo con función de pérdidad de más arriba\n"]},{"cell_type":"code","metadata":{"id":"sDOQ5IvFLxDD"},"source":["from tensorflow import keras\n","\n","model.compile(loss=yolo_loss ,optimizer='adam')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PlPF3nyX5yNu"},"source":["### Entrenar el modelo \n","\n","Entrenamos durante 135 epochs con `model.fit`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qqi3Vt_LzbC","outputId":"1a4c48aa-0430-4d3f-d832-8f1473750822"},"source":["model.fit(x=my_training_batch_generator,\n","          steps_per_epoch = int(len(X_train) // batch_size),\n","          epochs = 135,\n","          verbose = 1,\n","          workers= 4,\n","          validation_data = my_validation_batch_generator,\n","          validation_steps = int(len(X_val) // batch_size),\n","           callbacks=[\n","              CustomLearningRateScheduler(lr_schedule),\n","              mcp_save\n","          ])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Epoch 00000: Learning rate is 0.0100.\n","Epoch 1/135\n","625/625 [==============================] - 194s 310ms/step - loss: 63.7584 - val_loss: 59.6588\n","\n","Epoch 00001: Learning rate is 0.0100.\n","Epoch 2/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6997 - val_loss: 59.6588\n","\n","Epoch 00002: Learning rate is 0.0100.\n","Epoch 3/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7505 - val_loss: 59.6588\n","\n","Epoch 00003: Learning rate is 0.0100.\n","Epoch 4/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8414 - val_loss: 59.6588\n","\n","Epoch 00004: Learning rate is 0.0100.\n","Epoch 5/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7653 - val_loss: 59.6588\n","\n","Epoch 00005: Learning rate is 0.0100.\n","Epoch 6/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.9053 - val_loss: 59.6588\n","\n","Epoch 00006: Learning rate is 0.0100.\n","Epoch 7/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7567 - val_loss: 59.6588\n","\n","Epoch 00007: Learning rate is 0.0100.\n","Epoch 8/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.5225 - val_loss: 59.6588\n","\n","Epoch 00008: Learning rate is 0.0100.\n","Epoch 9/135\n","625/625 [==============================] - 188s 300ms/step - loss: 63.7042 - val_loss: 59.6588\n","\n","Epoch 00009: Learning rate is 0.0100.\n","Epoch 10/135\n","625/625 [==============================] - 188s 300ms/step - loss: 63.6403 - val_loss: 59.6588\n","\n","Epoch 00010: Learning rate is 0.0100.\n","Epoch 11/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8800 - val_loss: 59.6588\n","\n","Epoch 00011: Learning rate is 0.0100.\n","Epoch 12/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7961 - val_loss: 59.6588\n","\n","Epoch 00012: Learning rate is 0.0100.\n","Epoch 13/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7894 - val_loss: 59.6588\n","\n","Epoch 00013: Learning rate is 0.0100.\n","Epoch 14/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6460 - val_loss: 59.6588\n","\n","Epoch 00014: Learning rate is 0.0100.\n","Epoch 15/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7915 - val_loss: 59.6588\n","\n","Epoch 00015: Learning rate is 0.0100.\n","Epoch 16/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6158 - val_loss: 59.6588\n","\n","Epoch 00016: Learning rate is 0.0100.\n","Epoch 17/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6114 - val_loss: 59.6588\n","\n","Epoch 00017: Learning rate is 0.0100.\n","Epoch 18/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7119 - val_loss: 59.6588\n","\n","Epoch 00018: Learning rate is 0.0100.\n","Epoch 19/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6232 - val_loss: 59.6588\n","\n","Epoch 00019: Learning rate is 0.0100.\n","Epoch 20/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7963 - val_loss: 59.6588\n","\n","Epoch 00020: Learning rate is 0.0100.\n","Epoch 21/135\n","625/625 [==============================] - 189s 302ms/step - loss: 63.6088 - val_loss: 59.6588\n","\n","Epoch 00021: Learning rate is 0.0100.\n","Epoch 22/135\n","625/625 [==============================] - 189s 302ms/step - loss: 63.8363 - val_loss: 59.6588\n","\n","Epoch 00022: Learning rate is 0.0100.\n","Epoch 23/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6764 - val_loss: 59.6588\n","\n","Epoch 00023: Learning rate is 0.0100.\n","Epoch 24/135\n","625/625 [==============================] - 189s 302ms/step - loss: 63.6711 - val_loss: 59.6588\n","\n","Epoch 00024: Learning rate is 0.0100.\n","Epoch 25/135\n","625/625 [==============================] - 189s 302ms/step - loss: 63.8114 - val_loss: 59.6588\n","\n","Epoch 00025: Learning rate is 0.0100.\n","Epoch 26/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7982 - val_loss: 59.6588\n","\n","Epoch 00026: Learning rate is 0.0100.\n","Epoch 27/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7045 - val_loss: 59.6588\n","\n","Epoch 00027: Learning rate is 0.0100.\n","Epoch 28/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6001 - val_loss: 59.6588\n","\n","Epoch 00028: Learning rate is 0.0100.\n","Epoch 29/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6470 - val_loss: 59.6588\n","\n","Epoch 00029: Learning rate is 0.0100.\n","Epoch 30/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8486 - val_loss: 59.6588\n","\n","Epoch 00030: Learning rate is 0.0100.\n","Epoch 31/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.9907 - val_loss: 59.6588\n","\n","Epoch 00031: Learning rate is 0.0100.\n","Epoch 32/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7551 - val_loss: 59.6588\n","\n","Epoch 00032: Learning rate is 0.0100.\n","Epoch 33/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.5902 - val_loss: 59.6588\n","\n","Epoch 00033: Learning rate is 0.0100.\n","Epoch 34/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8138 - val_loss: 59.6588\n","\n","Epoch 00034: Learning rate is 0.0100.\n","Epoch 35/135\n","625/625 [==============================] - 189s 302ms/step - loss: 63.7984 - val_loss: 59.6588\n","\n","Epoch 00035: Learning rate is 0.0100.\n","Epoch 36/135\n","625/625 [==============================] - 189s 302ms/step - loss: 63.9033 - val_loss: 59.6588\n","\n","Epoch 00036: Learning rate is 0.0100.\n","Epoch 37/135\n","625/625 [==============================] - 188s 302ms/step - loss: 63.6496 - val_loss: 59.6588\n","\n","Epoch 00037: Learning rate is 0.0100.\n","Epoch 38/135\n","625/625 [==============================] - 189s 302ms/step - loss: 63.6613 - val_loss: 59.6588\n","\n","Epoch 00038: Learning rate is 0.0100.\n","Epoch 39/135\n","625/625 [==============================] - 189s 302ms/step - loss: 63.7350 - val_loss: 59.6588\n","\n","Epoch 00039: Learning rate is 0.0100.\n","Epoch 40/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6580 - val_loss: 59.6588\n","\n","Epoch 00040: Learning rate is 0.0100.\n","Epoch 41/135\n","625/625 [==============================] - 188s 302ms/step - loss: 63.7642 - val_loss: 59.6588\n","\n","Epoch 00041: Learning rate is 0.0100.\n","Epoch 42/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7213 - val_loss: 59.6588\n","\n","Epoch 00042: Learning rate is 0.0100.\n","Epoch 43/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7371 - val_loss: 59.6588\n","\n","Epoch 00043: Learning rate is 0.0100.\n","Epoch 44/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7973 - val_loss: 59.6588\n","\n","Epoch 00044: Learning rate is 0.0100.\n","Epoch 45/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6177 - val_loss: 59.6588\n","\n","Epoch 00045: Learning rate is 0.0100.\n","Epoch 46/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7342 - val_loss: 59.6588\n","\n","Epoch 00046: Learning rate is 0.0100.\n","Epoch 47/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7808 - val_loss: 59.6588\n","\n","Epoch 00047: Learning rate is 0.0100.\n","Epoch 48/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6850 - val_loss: 59.6588\n","\n","Epoch 00048: Learning rate is 0.0100.\n","Epoch 49/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7077 - val_loss: 59.6588\n","\n","Epoch 00049: Learning rate is 0.0100.\n","Epoch 50/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8597 - val_loss: 59.6588\n","\n","Epoch 00050: Learning rate is 0.0100.\n","Epoch 51/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6953 - val_loss: 59.6588\n","\n","Epoch 00051: Learning rate is 0.0100.\n","Epoch 52/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6610 - val_loss: 59.6588\n","\n","Epoch 00052: Learning rate is 0.0100.\n","Epoch 53/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7631 - val_loss: 59.6588\n","\n","Epoch 00053: Learning rate is 0.0100.\n","Epoch 54/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7437 - val_loss: 59.6588\n","\n","Epoch 00054: Learning rate is 0.0100.\n","Epoch 55/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6236 - val_loss: 59.6588\n","\n","Epoch 00055: Learning rate is 0.0100.\n","Epoch 56/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7268 - val_loss: 59.6588\n","\n","Epoch 00056: Learning rate is 0.0100.\n","Epoch 57/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.9170 - val_loss: 59.6588\n","\n","Epoch 00057: Learning rate is 0.0100.\n","Epoch 58/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8106 - val_loss: 59.6588\n","\n","Epoch 00058: Learning rate is 0.0100.\n","Epoch 59/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7058 - val_loss: 59.6588\n","\n","Epoch 00059: Learning rate is 0.0100.\n","Epoch 60/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8850 - val_loss: 59.6588\n","\n","Epoch 00060: Learning rate is 0.0100.\n","Epoch 61/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8984 - val_loss: 59.6588\n","\n","Epoch 00061: Learning rate is 0.0100.\n","Epoch 62/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7065 - val_loss: 59.6588\n","\n","Epoch 00062: Learning rate is 0.0100.\n","Epoch 63/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7719 - val_loss: 59.6588\n","\n","Epoch 00063: Learning rate is 0.0100.\n","Epoch 64/135\n","625/625 [==============================] - 188s 302ms/step - loss: 63.8356 - val_loss: 59.6588\n","\n","Epoch 00064: Learning rate is 0.0100.\n","Epoch 65/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8374 - val_loss: 59.6588\n","\n","Epoch 00065: Learning rate is 0.0100.\n","Epoch 66/135\n","625/625 [==============================] - 188s 301ms/step - loss: 64.0270 - val_loss: 59.6588\n","\n","Epoch 00066: Learning rate is 0.0100.\n","Epoch 67/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8506 - val_loss: 59.6588\n","\n","Epoch 00067: Learning rate is 0.0100.\n","Epoch 68/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8392 - val_loss: 59.6588\n","\n","Epoch 00068: Learning rate is 0.0100.\n","Epoch 69/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8326 - val_loss: 59.6588\n","\n","Epoch 00069: Learning rate is 0.0100.\n","Epoch 70/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7533 - val_loss: 59.6588\n","\n","Epoch 00070: Learning rate is 0.0100.\n","Epoch 71/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8216 - val_loss: 59.6588\n","\n","Epoch 00071: Learning rate is 0.0100.\n","Epoch 72/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7195 - val_loss: 59.6588\n","\n","Epoch 00072: Learning rate is 0.0100.\n","Epoch 73/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.5025 - val_loss: 59.6588\n","\n","Epoch 00073: Learning rate is 0.0100.\n","Epoch 74/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8365 - val_loss: 59.6588\n","\n","Epoch 00074: Learning rate is 0.0100.\n","Epoch 75/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7829 - val_loss: 59.6588\n","\n","Epoch 00075: Learning rate is 0.0010.\n","Epoch 76/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6690 - val_loss: 59.6588\n","\n","Epoch 00076: Learning rate is 0.0010.\n","Epoch 77/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8708 - val_loss: 59.6588\n","\n","Epoch 00077: Learning rate is 0.0010.\n","Epoch 78/135\n","625/625 [==============================] - 189s 302ms/step - loss: 63.8058 - val_loss: 59.6588\n","\n","Epoch 00078: Learning rate is 0.0010.\n","Epoch 79/135\n","625/625 [==============================] - 189s 302ms/step - loss: 63.8225 - val_loss: 59.6588\n","\n","Epoch 00079: Learning rate is 0.0010.\n","Epoch 80/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7678 - val_loss: 59.6588\n","\n","Epoch 00080: Learning rate is 0.0010.\n","Epoch 81/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.5811 - val_loss: 59.6588\n","\n","Epoch 00081: Learning rate is 0.0010.\n","Epoch 82/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.5787 - val_loss: 59.6588\n","\n","Epoch 00082: Learning rate is 0.0010.\n","Epoch 83/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8933 - val_loss: 59.6588\n","\n","Epoch 00083: Learning rate is 0.0010.\n","Epoch 84/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.4712 - val_loss: 59.6588\n","\n","Epoch 00084: Learning rate is 0.0010.\n","Epoch 85/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7654 - val_loss: 59.6588\n","\n","Epoch 00085: Learning rate is 0.0010.\n","Epoch 86/135\n","625/625 [==============================] - 188s 302ms/step - loss: 63.6758 - val_loss: 59.6588\n","\n","Epoch 00086: Learning rate is 0.0010.\n","Epoch 87/135\n","625/625 [==============================] - 188s 302ms/step - loss: 63.6720 - val_loss: 59.6588\n","\n","Epoch 00087: Learning rate is 0.0010.\n","Epoch 88/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7604 - val_loss: 59.6588\n","\n","Epoch 00088: Learning rate is 0.0010.\n","Epoch 89/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7144 - val_loss: 59.6588\n","\n","Epoch 00089: Learning rate is 0.0010.\n","Epoch 90/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.5538 - val_loss: 59.6588\n","\n","Epoch 00090: Learning rate is 0.0010.\n","Epoch 91/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7737 - val_loss: 59.6588\n","\n","Epoch 00091: Learning rate is 0.0010.\n","Epoch 92/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.5845 - val_loss: 59.6588\n","\n","Epoch 00092: Learning rate is 0.0010.\n","Epoch 93/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.6940 - val_loss: 59.6588\n","\n","Epoch 00093: Learning rate is 0.0010.\n","Epoch 94/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.8722 - val_loss: 59.6588\n","\n","Epoch 00094: Learning rate is 0.0010.\n","Epoch 95/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7744 - val_loss: 59.6588\n","\n","Epoch 00095: Learning rate is 0.0010.\n","Epoch 96/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.9308 - val_loss: 59.6588\n","\n","Epoch 00096: Learning rate is 0.0010.\n","Epoch 97/135\n","625/625 [==============================] - 188s 301ms/step - loss: 63.7612 - val_loss: 59.6588\n","\n","Epoch 00097: Learning rate is 0.0010.\n","Epoch 98/135\n","625/625 [==============================] - ETA: 0s - loss: 63.7479"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rxmdBXaE59az"},"source":["## Conclusión\n","\n","Esta implementación construye YOLO V1 de primeros principios. La implementación no obtiene los mismos resultados que el paper original porque falta el paso de pretraining con imágenes de imagenet (seccion 2.2 del paper de YOLO). En el paper se pre entrenan las 20 primeras capas convolucionales seguidas de una capa de Average Pooling y una capa Fully Connected. Se pre entrenó esa red por una semana hasta llegar a top-5 accuracy de 88% en el set de validación de Imagenet 2012. Luego se convierte la red para realizar detección, se agregan cuatro capas convolucionales y 2 fully connected con pesos aleatorios. Los pesos "]},{"cell_type":"code","metadata":{"id":"nm8UrPYg3Lnm"},"source":[""],"execution_count":null,"outputs":[]}]}
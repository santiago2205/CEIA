{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"End to End DL example in pytorch v2.0.ipynb","provenance":[],"mount_file_id":"1Ur7tW3rvzKBaqqd3ZxyaFRpJlHReN_0b","authorship_tag":"ABX9TyPTqAWUCCf1I22HmVeCnn7Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5L-AcBbnUS2","outputId":"d9d84df7-1fd7-4995-e42d-bad1a2083215"},"source":["!pip install stats\n","!pip install preprocessing\n","!pip install torchviz"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting stats\n","  Downloading stats-0.1.2a.tar.gz (127 kB)\n","\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 30 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 40 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 71 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 127 kB 5.2 MB/s \n","\u001b[?25hBuilding wheels for collected packages: stats\n","  Building wheel for stats (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for stats: filename=stats-0.1.2a0-py3-none-any.whl size=24297 sha256=72b1c27eac145ce45d7fce9712041253559fe29d086184d51cc6e0c6943e0f8c\n","  Stored in directory: /root/.cache/pip/wheels/e3/1c/58/620049eecc13fb5b4920470895e07a39f86e889a0a58b11976\n","Successfully built stats\n","Installing collected packages: stats\n","Successfully installed stats-0.1.2a0\n","Collecting preprocessing\n","  Downloading preprocessing-0.1.13-py3-none-any.whl (349 kB)\n","\u001b[K     |████████████████████████████████| 349 kB 5.1 MB/s \n","\u001b[?25hCollecting nltk==3.2.4\n","  Downloading nltk-3.2.4.tar.gz (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 21.5 MB/s \n","\u001b[?25hCollecting sphinx-rtd-theme==0.2.4\n","  Downloading sphinx_rtd_theme-0.2.4-py2.py3-none-any.whl (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 53.1 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.2.4->preprocessing) (1.15.0)\n","Building wheels for collected packages: nltk\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.2.4-py3-none-any.whl size=1367721 sha256=702d8d19d0f13ce2094a8b5a0a5e0d83cd13aed874ec161b706f1c45ce85999c\n","  Stored in directory: /root/.cache/pip/wheels/90/5e/9e/4cb46185f2a16c60e6fc524372ba7fef89ce3347734c8798b6\n","Successfully built nltk\n","Installing collected packages: sphinx-rtd-theme, nltk, preprocessing\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed nltk-3.2.4 preprocessing-0.1.13 sphinx-rtd-theme-0.2.4\n","Collecting torchviz\n","  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.9.0+cu102)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n","Building wheels for collected packages: torchviz\n","  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4151 sha256=75b0882e82d1dcb9fe6c2f276634eb457830257089e210203014ef98509e7d75\n","  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n","Successfully built torchviz\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5gMRDwZdo7Ud"},"source":["# Intro\n","* Bajamos dataset\n","* Analizamos de manera basica el dataset (Cantidad de filas, columnas, tipo del label (regresion, clasificacion binaria, clasificacion multiclase), etc)\n","* Limpiamos el dataset (remover o hacer algunos operaciones sobre lso nans)\n","* Ingenieria de features (correr tests de correlacion, encontrar buenas features para predecir salidas, etc.)\n","* Normalizar los datos\n","* Crear un modelo base-line que me permita obtener metricas iniciales de un modelo basico (en general un modelo lineal)\n","* Crear modelos mas complejos como no lineales con deep-learning"]},{"cell_type":"markdown","metadata":{"id":"S-UGaY_Po4kI"},"source":["## Load dataset"]},{"cell_type":"code","metadata":{"id":"DU9C4qotsjJb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cO8HeW96lTJj"},"source":["import numpy as np\n","import torch\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jiVeieZXsoGx"},"source":["dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Deep Learning/Clase_6/class_7_wine_dataset_v2.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QIrcMxWzmtDh"},"source":["dataset.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxZY8TKcn-pH"},"source":["dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RHj5K8qaoNW1"},"source":["Objetivo: crear un modelo para predecir la calidad del vino basandonos en los parámetros medidos\n"]},{"cell_type":"markdown","metadata":{"id":"1w3wYs_boPKg"},"source":["## Data analysis of the output"]},{"cell_type":"code","metadata":{"id":"gULskdg0oKqJ"},"source":["quality = dataset.quality.to_numpy()\n","fig, ax1 = plt.subplots(1,1)\n","ax1.hist(quality)\n","print(f\"Min value: {quality.min()}\")\n","print(f\"Max value: {quality.max()}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yw55wGhUob0R"},"source":["* Regresión lineal: no se pone nada al final\n","* Clasificación binaria: se pone una sigmoid al final\n","* Clasificación multiclase: se pone un softmax al final"]},{"cell_type":"code","metadata":{"id":"LLQTQGwsoS5Y"},"source":["# Empezamos con un caso de clasificación binaria\n","# Agregamos una feature (output) binarizando el quality segun si es menor/igual a 6 o mayor\n","dataset['quality_label'] = dataset.quality.apply(lambda label: 0 if label <=6 else 1)\n","print(len(dataset[dataset['quality_label'] == 0]))\n","print(len(dataset[dataset['quality_label'] == 1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Oy6uH_6oeS_"},"source":["dataset.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7qCevBfzoicJ"},"source":["## Clean dataset"]},{"cell_type":"code","metadata":{"id":"wVpleRUgofln"},"source":["nans = dataset[dataset.isna().sum(axis=1)==1]\n","print(nans.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ad6Poy8Vo0MQ"},"source":["ds = nans = dataset[dataset.isna().sum(axis=1)==0]\n","print(ds.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kwc4OD9Spxcf"},"source":["## Feature engineering"]},{"cell_type":"code","metadata":{"id":"dJbbdKoEpvy4"},"source":["ds.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vkA-cjzp3x_"},"source":["# Tengo que trasnformar todas las varaiabeles / columanas categoricas en algo que mi modelo peuda usar apra entrenar\n","\n","#Tratamiento para la variales type\n","ds.type.unique() #Mapear type a 2 variabels one hor encoding\n","\n","y = pd.get_dummies(ds.type, prefix=\"type\")\n","y.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G2qS1-F2qh8E"},"source":["#Concatenar las dumies al dataset principal\n","ds = pd.concat([ds, y], axis=1)\n","ds.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M6Y9Bn2UrE4I"},"source":["corr = ds.loc[:, (ds.columns != 'vendor_id') & (ds.columns != 'type') & (ds.columns != 'quality_label')].corr()\n","fig, ax1 = plt.subplots(1, figsize=(18,10))\n","import seaborn as sns\n","sns.set(style=\"ticks\", color_codes=True, font_scale=1.5)\n","sns.heatmap(corr, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt = \".2f\")\n","sns.set(font_scale=1.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"doLCBaY5wVmg"},"source":["fig, ax1 = plt.subplots(1, figsize=(18,10))\n","_ = ax1.boxplot([dataset.loc[dataset['quality_label'] == 0, 'alcohol'], \n","                 dataset.loc[dataset['quality_label'] == 1, 'alcohol']])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLx9tA3ey_Mo"},"source":["#Nos vamos a quedar con las variables que vamos a usar para hacer el fit del modelo\n","vendor_id = ds['vendor_id']\n","final_data = ds.drop(['Unnamed: 0', 'quality_label', 'quality', 'vendor_id', 'type'], axis=1, inplace=False)\n","final_data.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6DvT1CSzjS5"},"source":["final_data = final_data.to_numpy()\n","label = ds.quality_label.to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bArBSbtC1wAu"},"source":["## Normalizacion de datos\n"]},{"cell_type":"code","metadata":{"id":"p0G5st0b1yE4"},"source":["normalized_data = (final_data - np.min(final_data, axis=0)) / (np.max(final_data, axis=0)-np.min(final_data, axis=0))\n","print(np.max(normalized_data, axis=0))\n","print(np.min(normalized_data, axis=0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gvrRsd9H05pA"},"source":["## Dataset split"]},{"cell_type":"code","metadata":{"id":"E6TV1oy-04eQ"},"source":["n = final_data.shape[0]\n","idx = np.random.permutation(np.arange(0, n))\n","train_idx = idx[:int(0.8 * n)]\n","test_idx = idx[int(0.8 * n):]\n","assert len(train_idx) + len(test_idx) == len(idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7zkf_l2UcvSY"},"source":["vendor_id = vendor_id.to_numpy()\n","unique, indices, inversa = np.unique(vendor_id, return_index=True, return_inverse=True)\n","vendor_id_to_index = {key: value for key, value in zip(unique, inversa)} # mapeo de indices para layer embeddings\n","vendor_index = np.array([vendor_id_to_index[id] for id in vendor_id])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bbs-DgPl1YbZ"},"source":["vendor_index_train = vendor_index[train_idx]\n","X_train = normalized_data[train_idx,:]\n","y_train = label[train_idx]\n","\n","vendor_index_test = vendor_index[test_idx]\n","X_test = normalized_data[test_idx,:]\n","y_test = label[test_idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RcOImRk_3eEI"},"source":["print(vendor_index_train.shape)\n","print(X_train.shape)\n","print(y_train.shape)\n","print(vendor_index_test.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E5AH61Rw23Pa"},"source":["## Logistic regression"]},{"cell_type":"code","metadata":{"id":"6S55YNqe3pH-"},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics\n","from sklearn.ensemble import RandomForestClassifier\n","\n","lr_model = LogisticRegression(C=1, solver='sag', max_iter=1000)\n","lr_model.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zA8IYpED4yx6"},"source":["lr_test_scores = lr_model.predict_proba(X_test)\n","fpr, tpr, thresholds = metrics.roc_curve(y_test, lr_test_scores[:,1])\n","print(metrics.auc(fpr, tpr))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJRC45_w5t0S"},"source":["## Random Forest"]},{"cell_type":"code","metadata":{"id":"Ob5nGKIZ5Rh4"},"source":["rf_model = RandomForestClassifier(n_estimators=1000, max_depth=None, random_state=0)\n","rf_model.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qv3KQusm6ehG"},"source":["rf_test_score = rf_model.predict_proba(X_test)\n","fpr, tpr, thresholds = metrics.roc_curve(y_test, rf_test_score[:,1])\n","print(metrics.auc(fpr, tpr))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RCCP2IXi9e4U"},"source":["## Deep learning"]},{"cell_type":"code","metadata":{"id":"S8kguZhh6yoA"},"source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sq9-3gy6mjFP"},"source":["class CustomDataset(Dataset):\n","  def __init__(self, X, Y):\n","    super().__init__()\n","    self.X = X\n","    self.Y = Y\n","  \n","  def __len__(self):\n","    return self.X.shape[0]\n","  \n","  def __getitem__(self, idx):\n","    return self.X[idx,:], self.Y[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZG3vO9gq_Y0"},"source":["training = CustomDataset(X_train, y_train)\n","testing = CustomDataset(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FynxAZo1rJKs"},"source":["print(len(training))\n","print(len(testing))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZU-xw_ErOHb"},"source":["training_dataloader = DataLoader(training, batch_size=128, shuffle=True)\n","test_dataloader = DataLoader(testing, batch_size=128, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M2GWrBA3rkV8"},"source":["class NNet(torch.nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.linear_1 = torch.nn.Linear(in_features=13, out_features=20, bias=True)\n","    self.relu_1 = torch.nn.ReLU()\n","    self.linear_2 = torch.nn.Linear(in_features=20, out_features=5, bias=True)\n","    self.relu_2 = torch.nn.ReLU()\n","    self.linear_3 = torch.nn.Linear(in_features=5, out_features=1, bias=True)\n","  \n","  def forward(self, x):\n","    z1 = self.linear_1(x)\n","    a1 = self.relu_1(z1)\n","    z2 = self.linear_2(a1)\n","    a2 = self.relu_2(z2)\n","    y = self.linear_3(a2)\n","    return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z7CrlJjefUw5"},"source":["nnet = NNet()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pcF8L-PUfXba"},"source":["print(nnet)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b9_mbGT7fa-R"},"source":["criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n","optimizer = torch.optim.SGD(nnet.parameters(), lr=0.001)\n","torch.cuda.is_available()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"807M0WcsgCdz"},"source":["device = \"\"\n","if torch.cuda.is_available():\n","  device = \"cuda:0\"\n","else:\n","  device = \"cpu\"\n","\n","nnet.to(device)\n","\n","for epoch in range(100):\n","\n","  running_loss = 0\n","  nnet.train()\n","  for i, data in enumerate(training_dataloader):\n","\n","    # batch\n","    x, y = data\n","    x = x.to(device).float()\n","    y = y.to(device).float().reshape(-1,1)\n","\n","    # set gradient to zero\n","    optimizer.zero_grad()\n","\n","    # forward\n","    y_hat = nnet(x)\n","\n","    # loss\n","    loss = criterion(y_hat, y)\n","\n","    # backward\n","    loss.backward()\n","\n","    # update of parameters\n","    optimizer.step()\n","\n","    # compute metrics and statistics\n","    running_loss += loss.item()\n","  \n","  nnet.eval()\n","  nnet_test_score = []\n","  truth = []\n","  for i, data in enumerate(test_dataloader):\n","    # batch\n","    x, y = data\n","    x = x.to(device).float()\n","    y = y.to(device).float().reshape(-1,1)\n","\n","    # forward\n","    y_hat = nnet(x)\n","\n","    # accumulate data\n","    truth += list(y)\n","    nnet_test_score += list(y_hat)\n","\n","  fpr, tpr, thresholds = metrics.roc_curve(y_test, rf_test_score[:,1])\n","  auc = metrics.auc(fpr, tpr)\n","  print(f\"Epoch = {epoch} | loss = {running_loss / len(training)} | auc = {auc}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NTDyfYFOa2QQ"},"source":["## Deep learning con Embeddings"]},{"cell_type":"code","metadata":{"id":"sJ07E_IGbChV"},"source":["class CustomDatasetWithEmbedding(Dataset):\n","  def __init__(self, X, vendor_idx, Y):\n","    super().__init__()\n","    self.X = X\n","    self.vendor_idx = vendor_idx\n","    self.Y = Y\n","  \n","  def __len__(self):\n","    return self.X.shape[0]\n","  \n","  def __getitem__(self, idx):\n","    return self.X[idx,:], self.vendor_idx[idx], self.Y[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VEzo0FjplGgN"},"source":["training = CustomDatasetWithEmbedding(X_train, vendor_index_train, y_train)\n","testing = CustomDatasetWithEmbedding(X_test, vendor_index_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_lckv4MlaEC"},"source":["training_dataloader = DataLoader(training, batch_size=128, shuffle=True)\n","test_dataloader = DataLoader(testing, batch_size=128, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K72J83hVlgqE"},"source":["class NNet(torch.nn.Module):\n","    def __init__(self, number_of_vendors, embedding_dim):\n","        super().__init__()\n","        self.embedding = torch.nn.Embedding(num_embeddings=number_of_vendors, embedding_dim=embedding_dim)\n","        self.linear_1 = torch.nn.Linear(in_features=(13 + embedding_dim), out_features=200, bias=True)\n","        self.relu_1 = torch.nn.ReLU()\n","        self.linear_2 = torch.nn.Linear(in_features=200, out_features=100, bias=True)\n","        self.relu_2 = torch.nn.ReLU()\n","        self.linear_3 = torch.nn.Linear(in_features=100, out_features=1, bias=True)\n","    \n","    def forward(self, x, vendor_idx):\n","        vendor_emb = self.embedding(vendor_idx)\n","        final_input = torch.cat([x, vendor_emb], dim=1)\n","        z1 = self.linear_1(final_input)\n","        a1 = self.relu_1(z1)\n","        z2 = self.linear_2(a1)\n","        a2 = self.relu_2(z2)\n","        y = self.linear_3(a2)\n","        return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3i2eMTZSnNY7"},"source":["nnet = NNet(number_of_vendors=len(unique), embedding_dim=16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0egnnGlnqAP"},"source":["print(nnet)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpv3BBeNnrjZ"},"source":["criterion = torch.nn.BCEWithLogitsLoss(reduction='sum') # criterion, is my lost function\n","optimizer = torch.optim.Adam(nnet.parameters(), lr=0.01) # is my optimizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Toz-f6cAoOYB"},"source":["device = \"\"\n","if torch.cuda.is_available():\n","    device = \"cuda:0\"\n","else:\n","    device = \"cpu\"\n","\n","# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","nnet.to(device)\n","\n","for epoch in range(20):\n","\n","    running_loss = 0\n","    nnet.train()\n","    for i, data in enumerate(training_dataloader):\n","\n","        # batch\n","        x, vendor_idx, y = data\n","\n","        #vendor_idx = vendor_idx.reshape(-1,1)\n","        x = x.to(device).float()\n","        y = y.to(device).float().reshape(-1,1)\n","\n","        # set gradient to zero\n","        optimizer.zero_grad()\n","\n","        # forward \n","        y_hat = nnet(x, vendor_idx)\n","\n","        # loss\n","        loss = criterion(y_hat, y)\n","\n","        # backward\n","        loss.backward()\n","\n","        # update of parameters\n","        optimizer.step()\n","\n","        # compute metrics and statistics\n","        running_loss += loss.item()\n","    \n","    nnet.eval()\n","    with torch.no_grad():\n","        nnet_test_scores = []\n","        truth = []\n","        for i, data in enumerate(test_dataloader):\n","            # batch\n","            x, vendor_idx, y = data\n","            x = x.to(device).float()\n","            y = y.to(device).float().reshape(-1,1)\n","\n","            # forward \n","            y_hat = nnet(x, vendor_idx)\n","            y_hat = torch.sigmoid(y_hat)\n","\n","            # accumulate data\n","            truth += list(y.detach().numpy()) \n","            nnet_test_scores += list(y_hat.detach().numpy())\n","\n","        fpr, tpr, thresholds = metrics.roc_curve(truth, nnet_test_scores)\n","        auc = metrics.auc(fpr, tpr)\n","        print(f\"Epoch = {epoch} | loss = {running_loss / len(training)} | auc = {auc}\")"],"execution_count":null,"outputs":[]}]}